{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['Thomas',\n 'Jefferson',\n 'began',\n 'building',\n 'Monticello',\n 'at',\n 'the',\n 'age',\n 'of',\n '26.']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Thomas Jefferson began building Monticello at the age of 26.\"\n",
    "sentence.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "['Thomas',\n 'Jefferson',\n 'began',\n 'building',\n 'Monticello',\n 'at',\n 'the',\n 'age',\n 'of',\n '26.']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str.split(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "one-hot 编码"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_sequence = str.split(sentence)\n",
    "#词汇表（去掉重复项）\n",
    "vocab = sorted(set(token_sequence))\n",
    "' ,'.join(vocab)\n",
    "num_tokens = len(token_sequence)\n",
    "vocab_size = len(vocab)\n",
    "#one-hot 独热编码\n",
    "#长度：文档长度    宽度：词汇表中独立词项（不同）的个数\n",
    "onehot_vectors = np.zeros((num_tokens,vocab_size), int)\n",
    "onehot_vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n       [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#形成独热编码；enumerate:枚举\n",
    "for i,word in enumerate(token_sequence):\n",
    "    onehot_vectors[i, vocab.index(word)] = 1\n",
    "onehot_vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "'Thomas Jefferson began building Monticello at the age of 26.'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#文档词序列\n",
    "' '.join(token_sequence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'26. Jefferson Monticello Thomas age at began building of the'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#词汇表序列（按照大小已经排序了）\n",
    "' '.join(vocab)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "df = pd.DataFrame(onehot_vectors, columns=vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "  26. Jefferson Monticello Thomas age at began building of the\n0                               1                             \n1             1                                               \n2                                            1                \n3                                                     1       \n4                        1                                    \n5                                      1                      \n6                                                            1\n7                                   1                         \n8                                                        1    \n9   1                                                         ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>26.</th>\n      <th>Jefferson</th>\n      <th>Monticello</th>\n      <th>Thomas</th>\n      <th>age</th>\n      <th>at</th>\n      <th>began</th>\n      <th>building</th>\n      <th>of</th>\n      <th>the</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td>1</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1</td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n      <td></td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df == 0] = ' '\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "['26.',\n 'Jefferson',\n 'Monticello',\n 'Thomas',\n 'age',\n 'at',\n 'began',\n 'building',\n 'of',\n 'the']"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_bow = {}\n",
    "for token in sentence.split():\n",
    "    sentence_bow[token] = 1\n",
    "\n",
    "sorted(sentence_bow)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "'asdsd2'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'asdsd{}'.format(2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "{'sent0': {'Thomas': 1,\n  'Jefferson': 1,\n  'began': 1,\n  'building': 1,\n  'Monticello': 1,\n  'at': 1,\n  'the': 1,\n  'age': 1,\n  'of': 1,\n  '26.': 1},\n 'sent1': {'Construction': 1,\n  'was': 1,\n  'done': 1,\n  'mostly': 1,\n  'by': 1,\n  'local': 1,\n  'masons': 1,\n  'and': 1,\n  'carpenters.': 1},\n 'sent2': {'He': 1,\n  'moved': 1,\n  'into': 1,\n  'the': 1,\n  'South': 1,\n  'Pavilion': 1,\n  'in': 1,\n  '1770.': 1},\n 'sent3': {'Turning': 1,\n  'Monticello': 1,\n  'into': 1,\n  'a': 1,\n  'neoclassical': 1,\n  'masterpiece': 1,\n  'was': 1,\n  \"Jefferson's\": 1,\n  'obsession.': 1}}"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\\n\"\"\"\n",
    "sentences += \"\"\"Construction was done mostly by local masons and carpenters.\\n\"\"\"\n",
    "sentences += \"He moved into the South Pavilion in 1770.\\n\"\n",
    "sentences += \"\"\"Turning Monticello into a neoclassical masterpiece was Jefferson's obsession.\"\"\"\n",
    "corpus = {}\n",
    "for i, sent in enumerate(sentences.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())\n",
    "corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#每个句子中的词是否出现\n",
    "df = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T\n",
    "df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "       Thomas  Jefferson  began  building  Monticello  at  the  age  of  26.  \\\nsent0       1          1      1         1           1   1    1    1   1    1   \nsent1       0          0      0         0           0   0    0    0   0    0   \nsent2       0          0      0         0           0   0    1    0   0    0   \nsent3       0          0      0         0           1   0    0    0   0    0   \n\n       ...  South  Pavilion  in  1770.  Turning  a  neoclassical  masterpiece  \\\nsent0  ...      0         0   0      0        0  0             0            0   \nsent1  ...      0         0   0      0        0  0             0            0   \nsent2  ...      1         1   1      1        0  0             0            0   \nsent3  ...      0         0   0      0        1  1             1            1   \n\n       Jefferson's  obsession.  \nsent0            0           0  \nsent1            0           0  \nsent2            0           0  \nsent3            1           1  \n\n[4 rows x 32 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Thomas</th>\n      <th>Jefferson</th>\n      <th>began</th>\n      <th>building</th>\n      <th>Monticello</th>\n      <th>at</th>\n      <th>the</th>\n      <th>age</th>\n      <th>of</th>\n      <th>26.</th>\n      <th>...</th>\n      <th>South</th>\n      <th>Pavilion</th>\n      <th>in</th>\n      <th>1770.</th>\n      <th>Turning</th>\n      <th>a</th>\n      <th>neoclassical</th>\n      <th>masterpiece</th>\n      <th>Jefferson's</th>\n      <th>obsession.</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>sent0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>sent1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>sent2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>sent3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 32 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看sent0和其他句子之间的相似度\n",
    "#句子0和句子1没有重合词\n",
    "df = df.T\n",
    "df.sent0.dot(df.sent1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#句子0和句子1只有一个重合词\n",
    "df.sent0.dot(df.sent2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#句子0和句子2只有一个重合词\n",
    "df.sent0.dot(df.sent3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 找出句子0和句子3的重合词"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Thomas', 0),\n ('Jefferson', 0),\n ('began', 0),\n ('building', 0),\n ('Monticello', 1),\n ('at', 0),\n ('the', 0),\n ('age', 0),\n ('of', 0),\n ('26.', 0),\n ('Construction', 0),\n ('was', 0),\n ('done', 0),\n ('mostly', 0),\n ('by', 0),\n ('local', 0),\n ('masons', 0),\n ('and', 0),\n ('carpenters.', 0),\n ('He', 0),\n ('moved', 0),\n ('into', 0),\n ('South', 0),\n ('Pavilion', 0),\n ('in', 0),\n ('1770.', 0),\n ('Turning', 0),\n ('a', 0),\n ('neoclassical', 0),\n ('masterpiece', 0),\n (\"Jefferson's\", 0),\n ('obsession.', 0)]"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k,v) for (k,v) in (df.sent0 & df.sent3).items()]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Monticello', 1)]"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k,v) for (k,v) in (df.sent0 & df.sent3).items() if v]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 利用正则表达式对Monticello句子进行切分"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "['Thomas',\n 'Jefferson',\n 'began',\n 'building',\n 'Monticello',\n 'at',\n 'the',\n 'age',\n 'of',\n '26',\n '']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#这将会基于出现至少一次（注意正则表达式后面的‘+’）的空格或者标点符号来对句子进行切分\n",
    "# ‘+’表示必须匹配方括号内的一个或多个字符。\n",
    "# \\s 是一个预定义字符类的快捷表示，该字符类包括所有的空白符，如敲击空格键、制表键、回车产生的字符\n",
    "import re\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "tokens = re.split(r'[-\\s.,;!?]+', sentence)\n",
    "tokens\n",
    "#最后还空一个"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "[' ', 'the', ' ', 'age', ' ', 'of', ' ', '26', '.', '']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预编译compile（）\n",
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
    "tokens = pattern.split(sentence)\n",
    "tokens[-10:] #just the last 10 tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "['Thomas',\n 'Jefferson',\n 'began',\n 'building',\n 'Monticello',\n 'at',\n 'the',\n 'age',\n 'of',\n '26']"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#上述分割结果包含‘.’  如果不想包含的话(去掉字符)\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "tokens = pattern.split(sentence)\n",
    "[x for x in tokens if x and x not in '- \\t\\n.,;!?']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 分词器包RegexpTokenizer,没空白符"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "['Thomas',\n 'Jefferson',\n 'began',\n 'building',\n 'Monticello',\n 'at',\n 'the',\n 'age',\n 'of',\n '26',\n '.']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\n",
    "tokenizer.tokenize(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 分词器包TreebankWordTokenizer,没空白符"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "['Monticello',\n 'was',\n \"n't\",\n 'designated',\n 'as',\n 'UNESCO',\n 'World',\n 'Heritage',\n 'Site',\n 'until',\n '1987',\n '.']"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"\"\"Monticello wasn't designated as UNESCO World Heritage Site until 1987.\"\"\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 对社交网络（Twitter，Facebook）非规范文本进行分词\n",
    "casual_tokenize函数可以剥离文本中的用户名，也可以减少词条内的重复字数"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['RT', '@TJMonticello', 'Best', 'day', 'everrrrrrrr', 'at', 'Monicello', '.', 'Awesommmmmmmeeeeeeee', 'day', ':*)']\n"
     ]
    },
    {
     "data": {
      "text/plain": "['RT',\n 'Best',\n 'day',\n 'everrr',\n 'at',\n 'Monicello',\n '.',\n 'Awesommmeee',\n 'day',\n ':*)']"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.casual import casual_tokenize\n",
    "message = \"\"\"RT @TJMonticello Best day everrrrrrrr at Monicello. Awesommmmmmmeeeeeeee day :*)\"\"\"\n",
    "print(casual_tokenize(message))\n",
    "casual_tokenize(message, reduce_len=True, strip_handles=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 讲词汇表扩展到n-gram"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import re"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 原始的1-gram 分词器"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "['Thomas',\n 'Jefferson',\n 'began',\n 'building',\n 'Monticello',\n 'at',\n 'the',\n 'age',\n 'of',\n '26']"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"Thomas Jefferson began building Monticello at the age of 26.\"\"\"\n",
    "pattern = re.compile(r\"([-\\s.,;!?])+\")\n",
    "tokens = pattern.split(sentence)\n",
    "tokens = [x for x in tokens if x and x not in '- \\t\\n,.;!?']\n",
    "tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### nltk中的2-gram分词器的结果"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Thomas', 'Jefferson'),\n ('Jefferson', 'began'),\n ('began', 'building'),\n ('building', 'Monticello'),\n ('Monticello', 'at'),\n ('at', 'the'),\n ('the', 'age'),\n ('age', 'of'),\n ('of', '26')]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "list(ngrams(tokens, 2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### nltk中的3-gram分词器的结果"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[('Thomas', 'Jefferson', 'began'),\n ('Jefferson', 'began', 'building'),\n ('began', 'building', 'Monticello'),\n ('building', 'Monticello', 'at'),\n ('Monticello', 'at', 'the'),\n ('at', 'the', 'age'),\n ('the', 'age', 'of'),\n ('age', 'of', '26')]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "list(ngrams(tokens, 3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 停用词 a,the,an,and,of ,or"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 简单的停用词用法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'fire']\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['a', 'an', 'the', 'on', 'of', 'this', 'is']\n",
    "tokens = ['the', 'house', 'is', 'on', 'fire']\n",
    "tokens_without_stopwords = [x for x in tokens if x not in stop_words]\n",
    "print(tokens_without_stopwords)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### nltk中的停用词库"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\z1825\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "print(stop_words[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "['i', 'a', 's', 't', 'd', 'm', 'o', 'y']"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[sw for sw in stop_words if len(sw) == 1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 词汇表归一化"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 大小写转换"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house', 'visitor', 'center']\n"
     ]
    }
   ],
   "source": [
    "#简单的大小写转换\n",
    "tokens = ['House', \"Visitor\", \"Center\"]\n",
    "normalized_tokens = [x.lower() for x in tokens]\n",
    "print(normalized_tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 词干还原(还原为原来的词)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "'dish washer wash dish'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "' '.join([stemmer.stem(w).strip(\"'\") for w in \"dish washer's washed dishes\".split()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 词形归并\n",
    "尽管bank、banked、banking分别和河岸、汽车、金融有关，但如果使用了此感还原工具\n",
    "就会被同等对待\n",
    "需要告诉WordNetLemmatizer你感兴趣的词性"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\z1825\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mwordnet\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001B[0m\n  Searched in:\n    - 'C:\\\\Users\\\\z1825/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\z1825\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001B[0m in \u001B[0;36m__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     79\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 80\u001B[1;33m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mroot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'{}/{}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msubdir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mzip_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     81\u001B[0m                 \u001B[1;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    674\u001B[0m     \u001B[0mresource_not_found\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'\\n%s\\n%s\\n%s\\n'\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0msep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmsg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 675\u001B[1;33m     \u001B[1;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresource_not_found\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    676\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mwordnet\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001B[0m\n  Searched in:\n    - 'C:\\\\Users\\\\z1825/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\z1825\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-21-a764fa9ad1ee>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstem\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mWordNetLemmatizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mlemmatizer\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mWordNetLemmatizer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 4\u001B[1;33m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlemmatizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlemmatize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"better\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m#默认词性的'n'，表示名词\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      5\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlemmatizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlemmatize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"better\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpos\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"a\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m#表示形容词\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlemmatizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlemmatize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"good\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpos\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"a\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\stem\\wordnet.py\u001B[0m in \u001B[0;36mlemmatize\u001B[1;34m(self, word, pos)\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     39\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mlemmatize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mword\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpos\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mNOUN\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 40\u001B[1;33m         \u001B[0mlemmas\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mwordnet\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_morphy\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mword\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mpos\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     41\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mmin\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlemmas\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlen\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mif\u001B[0m \u001B[0mlemmas\u001B[0m \u001B[1;32melse\u001B[0m \u001B[0mword\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001B[0m in \u001B[0;36m__getattr__\u001B[1;34m(self, attr)\u001B[0m\n\u001B[0;32m    114\u001B[0m             \u001B[1;32mraise\u001B[0m \u001B[0mAttributeError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 116\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__load\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    117\u001B[0m         \u001B[1;31m# This looks circular, but its not, since __load() changes our\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    118\u001B[0m         \u001B[1;31m# __class__ to something new:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001B[0m in \u001B[0;36m__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     79\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     80\u001B[0m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mroot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'{}/{}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msubdir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mzip_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 81\u001B[1;33m                 \u001B[1;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[1;33m:\u001B[0m \u001B[1;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     82\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     83\u001B[0m         \u001B[1;31m# Load the corpus.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\corpus\\util.py\u001B[0m in \u001B[0;36m__load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     76\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m             \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 78\u001B[1;33m                 \u001B[0mroot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'{}/{}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msubdir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     79\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mLookupError\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     80\u001B[0m                 \u001B[1;32mtry\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mroot\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnltk\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdata\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfind\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'{}/{}'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msubdir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mzip_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Anaconda\\lib\\site-packages\\nltk\\data.py\u001B[0m in \u001B[0;36mfind\u001B[1;34m(resource_name, paths)\u001B[0m\n\u001B[0;32m    673\u001B[0m     \u001B[0msep\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'*'\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;36m70\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    674\u001B[0m     \u001B[0mresource_not_found\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'\\n%s\\n%s\\n%s\\n'\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0msep\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmsg\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msep\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 675\u001B[1;33m     \u001B[1;32mraise\u001B[0m \u001B[0mLookupError\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresource_not_found\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    676\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    677\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mwordnet\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001B[0m\n  Searched in:\n    - 'C:\\\\Users\\\\z1825/nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\share\\\\nltk_data'\n    - 'D:\\\\Anaconda\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\z1825\\\\AppData\\\\Roaming\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"better\")) #默认词性的'n'，表示名词\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) #表示形容词\n",
    "print(lemmatizer.lemmatize(\"good\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"goods\", pos=\"a\"))\n",
    "print(lemmatizer.lemmatize(\"goods\", pos=\"n\"))\n",
    "print(lemmatizer.lemmatize(\"goodness\", pos=\"n\"))\n",
    "print(lemmatizer.lemmatize(\"best\", pos=\"a\"))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}